# Model Evaluation Report

**Status:** Template - Will be generated after running evaluation

**Purpose:** This report will contain comprehensive evaluation results for trained behavior classification models.

---

## Overview

This report is automatically generated by `src/models/evaluate_models.py` and contains:

1. **Performance Summary** - Metrics for all 4 models (RF and SVM for ruminating and feeding)
2. **Best Model Selection** - Comparison and selection rationale for each behavior
3. **Detailed Metrics** - Confusion matrices, classification reports, ROC curves
4. **Success Criteria Evaluation** - Validation against project requirements
5. **Recommendations** - Next steps and production deployment guidance

---

## How to Generate This Report

### Step 1: Train Models

```bash
cd src/models
python train_behavior_classifiers.py --search-type randomized --n-iter 50
```

This will:
- Train 4 models (RF and SVM for ruminating and feeding)
- Perform hyperparameter tuning with 5-fold cross-validation
- Save models to `models/trained/`
- Generate training summary

### Step 2: Evaluate on Validation Set

```bash
python evaluate_models.py --split validation --save-best
```

This will:
- Evaluate all 4 models on validation set
- Generate confusion matrices and ROC/PR curves
- Create this comprehensive report
- Save best models to `models/ruminating_classifier.pkl` and `models/feeding_classifier.pkl`

### Step 3: Final Test Set Evaluation (Optional)

```bash
python evaluate_models.py --split test
```

This performs final evaluation on held-out test set.

---

## Expected Output Structure

After running evaluation, you'll find:

```
results/
├── model_evaluation_report_validation.md    # This report (validation set)
├── model_evaluation_report_test.md          # Test set report (if generated)
├── confusion_matrices/
│   ├── rf_ruminating_validation_confusion_matrix.png
│   ├── rf_feeding_validation_confusion_matrix.png
│   ├── svm_ruminating_validation_confusion_matrix.png
│   └── svm_feeding_validation_confusion_matrix.png
└── roc_curves/
    ├── rf_ruminating_validation_roc_curve.png
    ├── rf_ruminating_validation_pr_curve.png
    ├── rf_feeding_validation_roc_curve.png
    ├── rf_feeding_validation_pr_curve.png
    ├── svm_ruminating_validation_roc_curve.png
    ├── svm_ruminating_validation_pr_curve.png
    ├── svm_feeding_validation_roc_curve.png
    └── svm_feeding_validation_pr_curve.png
```

---

## Dependencies

This evaluation requires:

1. **Trained Models** (from Task #90 training script):
   - `models/trained/rf_ruminating_model.pkl`
   - `models/trained/rf_feeding_model.pkl`
   - `models/trained/svm_ruminating_model.pkl`
   - `models/trained/svm_feeding_model.pkl`

2. **Prepared Datasets** (from Task #89):
   - `data/processed/training_features.pkl`
   - `data/processed/validation_features.pkl`
   - `data/processed/test_features.pkl`

---

## Success Criteria

The evaluation script checks the following success criteria:

### Model Performance
- ✓ Validation F1-score > 0.75 for both ruminating and feeding
- ✓ Balanced precision and recall (within 10% of each other)
- ✓ Confusion matrix diagonal dominance (true positives > false positives/negatives)

### Model Comparison
- ✓ Best model selected for each behavior based on F1-score
- ✓ ROC-AUC scores documented
- ✓ Precision-Recall curves analyzed

### Production Readiness
- ✓ Models serialized and can be reloaded
- ✓ Inference time < 100ms per sample (typically < 1ms for both RF and SVM)
- ✓ Models work with new data without retraining

---

## Troubleshooting

### Error: Training data not found

```
FileNotFoundError: Training data not found at data/processed/training_features.pkl
```

**Solution:** Run Task #89 (Prepare Training Dataset) first to generate the required data files.

### Error: No trained models found

```
FileNotFoundError: No trained models found in models/trained/
```

**Solution:** Run `train_behavior_classifiers.py` first to train the models.

### Warning: Low F1-scores

If validation F1-scores are below 0.75:

1. Check data quality and class balance
2. Try different hyperparameter ranges
3. Consider feature engineering improvements
4. Verify sensor data quality

### Warning: Imbalanced precision-recall

If precision and recall differ by > 10%:

1. Adjust classification threshold using ROC/PR curves
2. Consider using `class_weight='balanced'` in models
3. Review false positive vs false negative costs

---

## Contact

For questions or issues with model training and evaluation, refer to:
- `src/models/train_behavior_classifiers.py` - Training script documentation
- `src/models/evaluate_models.py` - Evaluation script documentation
- `docs/behavioral_thresholds_literature_review.md` - Expected behavioral patterns

---

**Note:** This template will be replaced with actual evaluation results after running the evaluation script.
